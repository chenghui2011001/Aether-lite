#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SPI-JSCCè®­ç»ƒè„šæœ¬ï¼šè¯­ä¹‰æ˜ å°„+ä½ç½®ç¼–ç +å›¾åƒcodecçš„å®Œæ•´è®­ç»ƒ

ç‰¹ç‚¹ï¼š
- é›†æˆSPIæ¶æ„çš„ç«¯åˆ°ç«¯è®­ç»ƒ
- æ”¯æŒæ¸è¿›å¼è®­ç»ƒç­–ç•¥
- ä¸“ç”¨çš„SPIæŸå¤±å‡½æ•°
- éŸ³é¢‘éªŒè¯å’Œæ£€æŸ¥ç‚¹ä¿å­˜
"""

from __future__ import annotations

import os
import sys
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

import argparse
from dataclasses import dataclass
from typing import Dict, Tuple, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
import torchaudio
import numpy as np
from tqdm import tqdm

from utils.multi_stft_discriminator import create_adversarial_wave_loss
from models.spi_lite_jscc import SPI_LiteSpeechJSCC
from utils.channel_sim import ChannelSimulator
from utils.real_data_loader import create_combined_data_loader, create_aether_data_loader
from training.fargan_losses import multi_resolution_stft_loss
from training.enhanced_losses import create_enhanced_audio_loss
from utils.audio_visualizer import create_batch_comparison_plots, save_comparison_audio_samples


@dataclass
class SPITrainConfig:
    """SPIè®­ç»ƒé…ç½®"""

    data_root: str = "/home/bluestar/FARGAN/opus/dnn/torch/Aether-lite/data_expert_augmented_small200k"
    stage: int = 2  # 2: è¿ç»­JSCC; 3: Hash + JSCC; 4: Hash + bit-level noise
    enable_hash: bool = False  # Step1ç­–ç•¥ï¼šå…ˆå…³é—­Hashï¼Œæµ‹è¯•SPI+JSCC baseline
    # FARGAN é¢„è®­ç»ƒæƒé‡
    fargan_ckpt: Optional[str] = "/home/bluestar/FARGAN/opus/dnn/torch/Aether-lite/fargan_pt/fargan_sq1Ab_adv_50.pth"
    fargan_unfreeze_steps: int = 5000  # FARGANè§£å†»æ­¥æ•°
    batch_size: int = 8
    sequence_length: int = 200
    num_epochs: int = 5
    lr: float = 1e-4
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # æ£€æŸ¥ç‚¹ä¿å­˜
    output_dir: str = "./checkpoints_spi_jscc"
    save_every_steps: int = 500
    resume_from: Optional[str] = None

    # éŸ³é¢‘éªŒè¯ä¿å­˜
    save_audio: bool = True
    audio_save_dir: str = "./audio_samples_spi"
    save_audio_every_steps: int = 100
    max_audio_samples: int = 4

    # å¯è§†åŒ–ä¿å­˜
    save_visualization: bool = True
    visualization_save_dir: str = "./visualizations_spi"
    save_visualization_every_steps: int = 200
    max_visualization_samples: int = 3

    # SPIä¸“ç”¨å‚æ•°
    img_size: int = 64
    semantic_dim: int = 16
    d_z: int = 32  # æ‰©å¤§latentç©ºé—´
    d_s: int = 24  # å¯¹åº”è°ƒæ•´symbolç©ºé—´

    # æŸå¤±æƒé‡ - å¹²å‡€baselineé…ç½®
    # Step 1 baseline: åªä¿ç•™ STFT + ç‰¹å¾é‡å»ºï¼Œå…³é—­å¢å¼ºå’Œå¯¹æŠ—
    lambda_wave: float = 1.0      # STFTæŸå¤±æƒé‡ (ä¸»è¦æŸå¤±)
    lambda_stft: float = 1.0      # STFTæŸå¤±æƒé‡
    lambda_adv: float = 0.0       # å¯¹æŠ—æŸå¤±æƒé‡ (å…³é—­ä»¥å»ºç«‹baseline)
    lambda_spi: float = 0.5       # SPIä¸“ç”¨æŸå¤±æƒé‡

    # æ¸è¿›å¼è®­ç»ƒ
    use_progressive: bool = True  # å¯ç”¨æ¸è¿›å¼è®­ç»ƒ
    stage1_steps: int = 200       # Stage1: æ— JSCCæ­¥æ•° (å‡å°)
    stage2_steps: int = 400       # Stage2: æ— ä¿¡é“å™ªå£°æ­¥æ•° (å‡å°)
    stage3_steps: int = 600       # Stage3: Hash bottleneckæ­¥æ•° (å‡å°)

    # æŸå¤±å‡½æ•°é€‰æ‹© - å¹²å‡€baselineé…ç½®
    use_stft_loss: bool = True    # ä½¿ç”¨STFTæŸå¤±
    use_enhanced_losses: bool = False  # å…³é—­å¢å¼ºæŸå¤±ä»¥å»ºç«‹baseline
    f0_loss_weight: float = 1.0      # F0æŸå¤±æƒé‡(ç”¨äºåç»­å®éªŒ)
    perceptual_loss_weight: float = 0.1  # æ„ŸçŸ¥æŸå¤±æƒé‡(ç”¨äºåç»­å®éªŒ)

    # é€šé“é…ç½®
    snr_min_db: float = -5.0
    snr_max_db: float = 15.0

    # å¤šGPUæ”¯æŒ
    distributed: bool = False
    local_rank: int = 0
    world_size: int = 1


def save_checkpoint(
    model: SPI_LiteSpeechJSCC,
    optimizer: torch.optim.Optimizer,
    disc_optimizer: torch.optim.Optimizer,
    cfg: SPITrainConfig,
    epoch: int,
    global_step: int,
    output_dir: str,
) -> None:
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    os.makedirs(output_dir, exist_ok=True)

    checkpoint_path = os.path.join(output_dir, f"spi_checkpoint_step_{global_step:06d}.pth")

    checkpoint = {
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "disc_optimizer_state_dict": disc_optimizer.state_dict(),
        "epoch": epoch,
        "global_step": global_step,
        "config": {
            "stage": cfg.stage,
            "img_size": cfg.img_size,
            "semantic_dim": cfg.semantic_dim,
            "d_z": cfg.d_z,
            "d_s": cfg.d_s,
            "use_progressive": cfg.use_progressive,
        }
    }

    torch.save(checkpoint, checkpoint_path)
    print(f"SPI Checkpoint saved: {checkpoint_path}")


def load_checkpoint(
    checkpoint_path: str,
    model: SPI_LiteSpeechJSCC,
    optimizer: torch.optim.Optimizer,
    disc_optimizer: torch.optim.Optimizer,
    device: torch.device,
) -> Tuple[int, int]:
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    print(f"Loading SPI checkpoint from: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device)

    # å…¼å®¹æ€§åŠ è½½ï¼šå¤„ç†æ¨¡å‹ç»“æ„å˜åŒ–
    model_state = checkpoint["model_state_dict"]
    current_state = model.state_dict()

    # è¿‡æ»¤å‡ºåŒ¹é…çš„å‚æ•°
    filtered_state = {}
    for key, value in model_state.items():
        if key in current_state and current_state[key].shape == value.shape:
            filtered_state[key] = value
        else:
            print(f"Skipping parameter {key}: shape mismatch or not found")

    # åŠ è½½åŒ¹é…çš„å‚æ•°
    model.load_state_dict(filtered_state, strict=False)

    # æŠ¥å‘ŠåŠ è½½çŠ¶æ€
    loaded_params = len(filtered_state)
    total_params = len(model_state)
    print(f"Loaded {loaded_params}/{total_params} parameters from checkpoint")
    try:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        print("Optimizer state loaded successfully")
    except (ValueError, KeyError) as e:
        print(f"Cannot load optimizer state (will restart): {e}")
        print("This is normal when model structure changes")

    try:
        disc_optimizer.load_state_dict(checkpoint["disc_optimizer_state_dict"])
        print("Discriminator optimizer state loaded successfully")
    except (ValueError, KeyError) as e:
        print(f"Cannot load discriminator optimizer state (will restart): {e}")
        print("This is normal when model structure changes")

    epoch = checkpoint["epoch"]
    global_step = checkpoint["global_step"]

    print(f"Resumed from epoch {epoch}, step {global_step}")
    return epoch, global_step


def save_audio_samples(
    audio_real: torch.Tensor,
    audio_gen: torch.Tensor,
    cfg: SPITrainConfig,
    epoch: int,
    global_step: int,
    sample_rate: int = 16000,
) -> None:
    """ä¿å­˜éŸ³é¢‘æ ·æœ¬"""
    if not cfg.save_audio:
        return

    os.makedirs(cfg.audio_save_dir, exist_ok=True)

    batch_size = min(audio_real.size(0), cfg.max_audio_samples)

    for i in range(batch_size):
        real_sample = audio_real[i].detach().cpu()
        gen_sample = audio_gen[i].detach().cpu()

        min_len = min(real_sample.size(0), gen_sample.size(0))
        real_sample = real_sample[:min_len]
        gen_sample = gen_sample[:min_len]

        # å½’ä¸€åŒ–
        real_sample = real_sample / (real_sample.abs().max() + 1e-8)
        gen_sample = gen_sample / (gen_sample.abs().max() + 1e-8)

        # ä¿å­˜æ–‡ä»¶
        real_path = os.path.join(
            cfg.audio_save_dir,
            f"spi_step_{global_step:06d}_sample_{i:02d}_real.wav"
        )
        gen_path = os.path.join(
            cfg.audio_save_dir,
            f"spi_step_{global_step:06d}_sample_{i:02d}_gen.wav"
        )

        torchaudio.save(real_path, real_sample.unsqueeze(0), sample_rate)
        torchaudio.save(gen_path, gen_sample.unsqueeze(0), sample_rate)

    print(f"SPI Audio samples saved: {batch_size} pairs at step {global_step}")


def determine_training_stage(global_step: int, cfg: SPITrainConfig) -> str:
    """ç¡®å®šå½“å‰è®­ç»ƒé˜¶æ®µ"""
    if not cfg.use_progressive:
        return f"stage{cfg.stage}"

    if global_step < cfg.stage1_steps:
        return "no_jscc"
    elif global_step < cfg.stage1_steps + cfg.stage2_steps:
        return "no_channel"
    elif global_step < cfg.stage1_steps + cfg.stage2_steps + cfg.stage3_steps:
        return "stage3_hash"
    else:
        return "stage4_hash_noise"


def forward_spi_progressive(
    model,  # å¯èƒ½æ˜¯ SPI_LiteSpeechJSCC æˆ– DistributedDataParallel
    feats: torch.Tensor,
    audio: torch.Tensor,
    channel_sim: ChannelSimulator,
    stage: str,
    cfg: SPITrainConfig,
    device: torch.device,
) -> Dict[str, torch.Tensor]:
    """
    SPIæ¸è¿›å¼å‰å‘ä¼ æ’­

    Args:
        model: SPIæ¨¡å‹
        feats: [B, T, 20] è¾“å…¥ç‰¹å¾
        audio: [B, L] éŸ³é¢‘
        channel_sim: ä¿¡é“æ¨¡æ‹Ÿå™¨
        stage: è®­ç»ƒé˜¶æ®µ
        cfg: é…ç½®
        device: è®¾å¤‡

    Returns:
        åŒ…å«æ‰€æœ‰è¾“å‡ºçš„å­—å…¸
    """
    B, T, _ = feats.shape

    # ç”ŸæˆCSI
    csi_dict, amp_t, snr_db_t = channel_sim.sample_csi(
        B, T, channel="fading", snr_min_db=cfg.snr_min_db, snr_max_db=cfg.snr_max_db
    )
    csi_vec = torch.stack([
        csi_dict["snr_proxy"],
        csi_dict["time_selectivity"],
        csi_dict["freq_selectivity"],
        csi_dict["los_ratio"],
    ], dim=-1).to(device=device, dtype=feats.dtype)

    # è·å–å®é™…æ¨¡å‹ (å¤„ç†DDPåŒ…è£…)
    actual_model = model.module if hasattr(model, 'module') else model

    if stage == "no_jscc":
        # Stage 1: ç›´æ¥ç¼–ç -è§£ç ï¼Œä¸ç»è¿‡JSCC
        encode_out = actual_model.spi_encode(feats, csi_vec)
        decode_out = actual_model.spi_decode(encode_out['z_encoded_multi'], csi_vec, encode_out['semantic_vec'], encode_out.get('temporal_info'))

        output = {
            **encode_out,
            **decode_out,
            'stage': stage,
            'feats_hat': decode_out['feats_recovered'],
        }

    elif stage == "no_channel":
        # Stage 2: å¤štoken JSCCä½†æ— ä¿¡é“å™ªå£° - ä¿®å¤ï¼šçœŸæ­£çš„å¤štokenå¤„ç†
        encode_out = actual_model.spi_encode(feats, csi_vec)
        z_multi = encode_out['z_encoded_multi']  # [B, N_patches, d_z]
        z = encode_out['z_encoded']              # [B, d_z] ç”¨äºå…¼å®¹
        semantic_vec = encode_out['semantic_vec']

        # å¤štoken JSCC ç¼–ç è§£ç ï¼ˆä¸åŠ å™ªå£°ï¼‰
        s_multi = actual_model.jscc_enc(z_multi, csi_vec)        # [B, N_patches, d_s]
        z_hat_multi = actual_model.jscc_dec(s_multi, csi_vec)    # [B, N_patches, d_z]

        # å•tokenä½œä¸ºsummaryï¼ˆç”¨äºå…¼å®¹/æ—¥å¿—ï¼‰
        z_hat = z_hat_multi.mean(dim=1) if z_hat_multi.size(1) > 1 else z_hat_multi.squeeze(1)

        decode_out = actual_model.spi_decode(
            z_hat_multi,                         # ğŸ‘ˆ ä½¿ç”¨å®Œæ•´å¤štoken
            csi_vec,
            semantic_vec,
            encode_out.get('temporal_info'),
            z_decoded_single=z_hat               # å•tokenä½œä¸ºå¤‡ç”¨
        )
        feat_hat = decode_out['feats_recovered']

        output = {
            **encode_out,
            **decode_out,
            'z': z,
            'z_hat': z_hat,
            'z_multi': z_multi,
            'z_hat_multi': z_hat_multi,
            's_multi': s_multi,
            'stage': stage,
            'feats_hat': feat_hat,
        }

    elif stage == "stage3_hash":
        # Stage 3: å¤štoken JSCC + è½»é‡channelå™ªå£°ï¼Œå‡†å¤‡æ¥å…¥Hashï¼ˆStep1ç­–ç•¥ï¼šå…ˆç¨³å®šJSCCï¼‰
        encode_out = actual_model.spi_encode(feats, csi_vec)
        z_multi = encode_out['z_encoded_multi']  # [B, N_patches, d_z]
        z = encode_out['z_encoded']              # [B, d_z] ç”¨äºå…¼å®¹
        semantic_vec = encode_out['semantic_vec']

        # å¤štoken JSCC ç¼–ç 
        s_multi = actual_model.jscc_enc(z_multi, csi_vec)   # [B, N_patches, d_s]

        # è½»é‡ä¿¡é“å™ªå£°ï¼ˆæ¯”åŸæ¥æ¸©å’Œï¼‰
        csi_dict2, amp_t2, snr_db_t2 = channel_sim.sample_csi(
            B, z_multi.size(1), channel="fading", snr_min_db=cfg.snr_min_db, snr_max_db=cfg.snr_max_db
        )
        amp_t2 = amp_t2.to(device=s_multi.device, dtype=s_multi.dtype)
        snr_db_t2 = snr_db_t2.to(device=s_multi.device, dtype=s_multi.dtype)
        s_multi_noisy = channel_sim.apply(s_multi, amp_t2, snr_db_t2)  # [B, N_patches, d_s]

        # å¤štoken JSCC è§£ç 
        z_hat_multi = actual_model.jscc_dec(s_multi_noisy, csi_vec)       # [B, N_patches, d_z]
        z_hat = z_hat_multi.mean(dim=1) if z_hat_multi.size(1) > 1 else z_hat_multi.squeeze(1)  # [B, d_z]

        # Hash bottleneck (no bit noise) - åœ¨JSCCå¤„ç†åçš„å¤štokenä¸Šæ“ä½œ
        hash_output = None  # æå‰å®šä¹‰ï¼Œé¿å…æœªå¯ç”¨hashæ—¶NameError

        if hasattr(model, 'hash') and actual_model.hash is not None:
            # ä½¿ç”¨JSCCå¤„ç†åçš„å¤štoken latentsï¼Œè€Œä¸æ˜¯åŸå§‹ç¼–ç 
            if z_hat_multi is not None:
                # è·å–patch mask
                temporal_info = encode_out.get('temporal_info')
                patch_mask = temporal_info.get('patch_mask') if temporal_info else None
                hash_output = actual_model.hash(z_hat_multi, channel_params=None, mask=patch_mask)  # [B, N_patches, d_z]
                z_hash_multi = hash_output['reconstructed']  # [B, N_patches, d_z]
                hash_bits_clean = hash_output['hash_bits_clean']  # [B, N_patches, hash_bits]
                hash_logits = hash_output['hash_logits']  # [B, N_patches, hash_bits]
                # ä¸ºäº†å…¼å®¹å•tokenæ¥å£ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªpatchä½œä¸ºä»£è¡¨
                z_hash = z_hash_multi[:, 0, :]  # [B, d_z]
                hash_mask = hash_output.get('mask')
            else:
                # å›é€€åˆ°å•tokenå¤„ç†
                hash_output = actual_model.hash(z_hat.unsqueeze(1), channel_params=None)  # [B, 1, d_z]
                z_hash = hash_output['reconstructed'].squeeze(1)  # [B, d_z]
                hash_bits_clean = hash_output['hash_bits_clean'].squeeze(1)  # [B, hash_bits]
                hash_logits = hash_output['hash_logits'].squeeze(1)  # [B, hash_bits]
                hash_mask = hash_output.get('mask')
        else:
            z_hash = z_hat
            z_hash_multi = encode_out.get('z_encoded_multi', z_hat.unsqueeze(1))
            hash_bits_clean = None
            hash_logits = None
            hash_mask = None

        # SPI è§£ç  - ä¿®å¤ï¼šä½¿ç”¨å®Œæ•´å¤štokenè€Œä¸æ˜¯åªç”¨ç¬¬ä¸€ä¸ª
        decode_out = actual_model.spi_decode(
            z_hash_multi,                    # ğŸ‘ˆ ä½¿ç”¨å®Œæ•´å¤štokenä½œä¸ºä¸»å¹²
            csi_vec,
            semantic_vec,
            encode_out.get('temporal_info'),
            z_decoded_single=z_hash          # å•tokenä½œä¸ºå¤‡ç”¨ï¼Œä¸æ˜¯ä¸»å¹²
        )
        feat_hat = decode_out['feats_recovered']

        output = {
            **encode_out,
            **decode_out,
            'z': z,
            'z_hat': z_hat,
            'z_multi': z_multi,
            'z_hat_multi': z_hat_multi,
            'z_hash': z_hash,
            'z_hash_multi': z_hash_multi,
            's_multi': s_multi,
            's_multi_noisy': s_multi_noisy,
            'hash_bits_clean': hash_bits_clean,
            'hash_logits': hash_logits,
            'hash_mask': hash_mask,
            'stage': stage,
            'feats_hat': feat_hat,
            'actual_snr': 10 * torch.log10(
                torch.mean(s_multi.pow(2)) / (torch.mean((s_multi_noisy - s_multi).pow(2)) + 1e-8)
            ),
        }

    elif stage == "stage4_hash_noise":
        # Stage 4: å¤štoken JSCC + ä¿¡é“å™ªå£° + Hash + bitå™ªå£° (å®Œå…¨ä¸²è”)
        encode_out = actual_model.spi_encode(feats, csi_vec)
        z_multi = encode_out['z_encoded_multi']  # [B, N_patches, d_z]
        semantic_vec = encode_out['semantic_vec']
        temporal_info = encode_out.get('temporal_info')

        # 1) å¤štoken JSCC ç¼–ç 
        s_multi = actual_model.jscc_enc(z_multi, csi_vec)    # [B, N_patches, d_s]

        # 2) å¤štoken ä¿¡é“å™ªå£°
        csi_dict2, amp_t2, snr_db_t2 = channel_sim.sample_csi(
            B, z_multi.size(1),
            channel="fading",
            snr_min_db=cfg.snr_min_db,
            snr_max_db=cfg.snr_max_db
        )
        amp_t2 = amp_t2.to(device=s_multi.device, dtype=s_multi.dtype)
        snr_db_t2 = snr_db_t2.to(device=s_multi.device, dtype=s_multi.dtype)
        s_multi_noisy = channel_sim.apply(s_multi, amp_t2, snr_db_t2)

        # 3) å¤štoken JSCC è§£ç 
        z_hat_multi = actual_model.jscc_dec(s_multi_noisy, csi_vec)   # [B, N_patches, d_z]

        # 4) Hash bottleneck + bit å™ªå£°ï¼ˆåœ¨ JSCC è¾“å‡ºä¸Šï¼‰
        hash_output = None  # æå‰å®šä¹‰ï¼Œé¿å…æœªå¯ç”¨hashæ—¶NameError

        if hasattr(model, 'hash') and actual_model.hash is not None:
            base_ber = 0.01
            max_ber = 0.15
            warm_frac = 0.3  # ç®€å•å…ˆå†™æ­»
            scheduled_ber = base_ber + warm_frac * (max_ber - base_ber)
            channel_params = {'ber': scheduled_ber}

            patch_mask = temporal_info.get('patch_mask') if temporal_info else None
            hash_output = actual_model.hash(z_hat_multi, channel_params=channel_params, mask=patch_mask)
            z_hash_multi = hash_output['reconstructed']
            hash_bits_clean = hash_output['hash_bits_clean']
            hash_bits_noisy = hash_output['hash_bits_noisy']
            hash_logits = hash_output['hash_logits']
            hash_mask = hash_output.get('mask')
        else:
            z_hash_multi = z_hat_multi
            hash_bits_clean = hash_bits_noisy = hash_logits = None
            hash_mask = None

        # 5) SPI è§£ç ï¼ˆå¤štokenä¸»å¹²ï¼‰
        decode_out = actual_model.spi_decode(
            z_hash_multi,
            csi_vec,
            semantic_vec,
            temporal_info
        )
        feat_hat = decode_out['feats_recovered']

        output = {
            **encode_out,
            **decode_out,
            'z_multi': z_multi,
            'z_hat_multi': z_hat_multi,
            'z_hash_multi': z_hash_multi,
            's_multi': s_multi,
            's_multi_noisy': s_multi_noisy,
            'hash_bits_clean': hash_bits_clean,
            'hash_bits_noisy': hash_bits_noisy,
            'hash_logits': hash_logits,
            'hash_mask': hash_mask,
            'stage': stage,
            'feats_hat': feat_hat,
            'actual_snr': 10 * torch.log10(
                torch.mean(s_multi.pow(2)) / (torch.mean((s_multi_noisy - s_multi).pow(2)) + 1e-8)
            ),
        }

    # é€šè¿‡vocoderç”ŸæˆéŸ³é¢‘
    feat_hat = output['feats_hat']
    target_len = audio.size(-1)

    try:
        period, audio_hat = actual_model.vocoder(feat_hat, target_len=target_len)
        audio_hat = audio_hat.squeeze(1) if audio_hat.dim() > 2 else audio_hat

        # é•¿åº¦å¯¹é½
        if audio_hat.size(-1) != audio.size(-1):
            min_len = min(audio_hat.size(-1), audio.size(-1))
            audio_hat = audio_hat[..., :min_len]
            audio = audio[..., :min_len]

        output.update({
            'audio_hat': audio_hat,
            'audio_real': audio,
            'period': period,
        })

    except Exception as e:
        print(f"Vocoder error: {e}")
        # åˆ›å»ºdummyéŸ³é¢‘
        audio_hat = torch.zeros_like(audio)
        output.update({
            'audio_hat': audio_hat,
            'audio_real': audio,
            'period': None,
        })

    return output


class ProgressiveLossScheduler:
    """
    æ¸è¿›å¼æŸå¤±æƒé‡è°ƒåº¦å™¨ - Anti-Buzzè®­ç»ƒç­–ç•¥

    å®ç°åˆ†é˜¶æ®µçš„æŸå¤±æƒé‡è°ƒæ•´ç­–ç•¥ï¼š
    - Stage 1 (0-1000): ä¼˜å…ˆç‰¹å¾çº§ç›‘ç£ï¼Œå…³é—­GAN
    - Stage 2 (1000-3000): åŠ å…¥éŸ³é¢‘çº§ç›‘ç£ï¼Œå¼±GAN
    - Stage 3 (3000-5000): é€æ­¥æ¢å¤GANæƒé‡
    - Stage 4 (5000+): è§£å†»FARGANï¼Œç«¯åˆ°ç«¯ä¼˜åŒ–
    """

    def __init__(
        self,
        stage1_steps: int = 1000,
        stage2_steps: int = 3000,
        stage3_steps: int = 5000,
        max_adv_weight: float = 0.3,
        max_spi_weight: float = 1.0
    ):
        self.stage1_steps = stage1_steps
        self.stage2_steps = stage2_steps
        self.stage3_steps = stage3_steps
        self.max_adv_weight = max_adv_weight
        self.max_spi_weight = max_spi_weight

    def get_weights(self, global_step: int) -> Dict[str, float]:
        """
        æ ¹æ®è®­ç»ƒæ­¥æ•°è¿”å›å½“å‰çš„æŸå¤±æƒé‡

        Args:
            global_step: å½“å‰è®­ç»ƒæ­¥æ•°

        Returns:
            æƒé‡å­—å…¸
        """
        weights = {
            'lambda_adv': 0.0,
            'lambda_spi': self.max_spi_weight,
            'lambda_wave': 1.0,
            'fargan_freeze': True,
            'stage_name': 'unknown'
        }

        if global_step < self.stage1_steps:
            # Stage 1: ä¼˜å…ˆç‰¹å¾çº§ç›‘ç£
            weights.update({
                'lambda_adv': 0.0,
                'lambda_spi': self.max_spi_weight,
                'fargan_freeze': True,
                'stage_name': 'feature_focus'
            })

        elif global_step < self.stage2_steps:
            # Stage 2: åŠ å…¥éŸ³é¢‘çº§ç›‘ç£ï¼Œå¼±GAN
            weights.update({
                'lambda_adv': 0.1 * self.max_adv_weight,
                'lambda_spi': self.max_spi_weight,
                'fargan_freeze': True,
                'stage_name': 'audio_weak_gan'
            })

        elif global_step < self.stage3_steps:
            # Stage 3: é€æ­¥æ¢å¤GANæƒé‡
            progress = (global_step - self.stage2_steps) / (self.stage3_steps - self.stage2_steps)
            weights.update({
                'lambda_adv': progress * self.max_adv_weight,
                'lambda_spi': self.max_spi_weight,
                'fargan_freeze': True,
                'stage_name': 'progressive_gan'
            })

        else:
            # Stage 4: è§£å†»FARGANï¼Œç«¯åˆ°ç«¯ä¼˜åŒ–
            weights.update({
                'lambda_adv': self.max_adv_weight,
                'lambda_spi': self.max_spi_weight * 0.8,  # ç•¥å¾®é™ä½SPIæƒé‡
                'fargan_freeze': False,
                'stage_name': 'end_to_end'
            })

        return weights


def compute_spi_training_loss(
    output: Dict,
    cfg: SPITrainConfig,
    adv_wave_loss,
    enhanced_loss,
    device: torch.device,
    model = None,  # å¯èƒ½æ˜¯ SPI_LiteSpeechJSCC æˆ– DistributedDataParallel
    global_step: int = 0,
    loss_scheduler: Optional[ProgressiveLossScheduler] = None
) -> Tuple[torch.Tensor, Dict]:
    """è®¡ç®—SPIè®­ç»ƒæŸå¤±ï¼ˆé›†æˆAnti-Buzzå’Œæ¸è¿›å¼è°ƒåº¦ï¼‰"""

    audio_hat = output['audio_hat']
    audio_real = output['audio_real']
    feats_hat = output['feats_hat']
    feats = output.get('feats', None)

    # è·å–å®é™…æ¨¡å‹ (å¤„ç†DDPåŒ…è£…)
    actual_model = model.module if hasattr(model, 'module') else model

    losses = {}

    # è·å–æ¸è¿›å¼æƒé‡
    if loss_scheduler is not None:
        schedule_weights = loss_scheduler.get_weights(global_step)
        current_lambda_adv = schedule_weights['lambda_adv']
        current_lambda_spi = schedule_weights['lambda_spi']
        stage_name = schedule_weights['stage_name']
    else:
        current_lambda_adv = cfg.lambda_adv
        current_lambda_spi = cfg.lambda_spi
        stage_name = 'default'

    # 1. åŸºç¡€éŸ³é¢‘æŸå¤±
    if cfg.use_stft_loss:
        # å¤šåˆ†è¾¨ç‡STFTæŸå¤±
        loss_stft = multi_resolution_stft_loss(
            audio_hat, audio_real,
            device=device,
            fft_sizes=[1024, 512, 256, 128],
            hop_sizes=[256, 128, 64, 32],
            win_lengths=[1024, 512, 256, 128]
        )
        losses['recon'] = loss_stft
        recon_type = "stft"
    else:
        # ä¼ ç»ŸL1æŸå¤±
        losses['recon'] = F.l1_loss(audio_hat, audio_real)
        recon_type = "l1"

    # 2. å¢å¼ºéŸ³é¢‘æŸå¤± (F0 + æ„ŸçŸ¥æŸå¤±) - ä»…åœ¨Stage2+å¯ç”¨
    if enhanced_loss is not None and cfg.use_enhanced_losses and global_step >= 1000:
        try:
            enhanced_losses = enhanced_loss(audio_hat, audio_real)

            # å°†å„ç»„ä»¶æŸå¤±æ·»åŠ åˆ°æ€»æŸå¤±ä¸­
            for key, value in enhanced_losses.items():
                if key != 'enhanced_total':  # é¿å…é‡å¤è®¡ç®—æ€»æŸå¤±
                    losses[f'enhanced_{key}'] = value

            # æ€»çš„å¢å¼ºæŸå¤±
            losses['enhanced'] = enhanced_losses['enhanced_total']

        except Exception as e:
            print(f"Enhanced loss computation failed: {e}")
            losses['enhanced'] = torch.zeros(1, device=device)
    else:
        losses['enhanced'] = torch.zeros(1, device=device)

    # 3. å¯¹æŠ—æŸå¤± (æ ¹æ®è°ƒåº¦å™¨æ§åˆ¶)
    if adv_wave_loss is not None and current_lambda_adv > 0:
        try:
            gen_out = adv_wave_loss.generator_step(audio_real, audio_hat)
            losses['adv'] = gen_out["total_adversarial_loss"]
        except:
            losses['adv'] = torch.zeros(1, device=device)
    else:
        losses['adv'] = torch.zeros(1, device=device)

    # 4. SPIä¸“ç”¨æŸå¤±ï¼ˆé›†æˆAnti-BuzzåŠŸèƒ½ï¼‰
    if feats is not None and model is not None:
        # ä½¿ç”¨æ¨¡å‹çš„é›†æˆAnti-BuzzæŸå¤±è®¡ç®—
        spi_loss_total, spi_losses = actual_model.compute_spi_loss(
            output, feats, audio_hat, audio_real
        )
        losses['spi_total'] = spi_loss_total

        # æ·»åŠ è¯¦ç»†çš„SPIæŸå¤±ç»„ä»¶
        for key, value in spi_losses.items():
            if key != 'spi_total':
                losses[f'spi_{key}'] = value

    else:
        # å¤‡ç”¨ï¼šç®€å•ç‰¹å¾é‡å»ºæŸå¤±
        if feats is not None:
            losses['feat_recon'] = F.mse_loss(feats_hat, feats)
            losses['spi_total'] = losses['feat_recon']
        else:
            losses['spi_total'] = torch.zeros(1, device=device)

    # 5. Hash bottleneckæ­£åˆ™åŒ–æŸå¤± (å¦‚æœæœ‰hashç›¸å…³è¾“å‡º)
    hash_logits = output.get('hash_logits')
    hash_bits_clean = output.get('hash_bits_clean')
    hash_mask = output.get('hash_mask')

    if hash_logits is not None and hash_bits_clean is not None and hasattr(model, 'hash') and actual_model.hash is not None:
        # ä½¿ç”¨HashBottleneckçš„æ–°APIè®¡ç®—æ­£åˆ™åŒ–æŸå¤±ï¼Œæ”¯æŒmask
        hash_reg_losses = actual_model.hash.compute_hash_regularization(
            hash_logits, hash_bits_clean, mask=hash_mask
        )

        # æå–å„é¡¹æŸå¤±
        losses['bit_balance'] = hash_reg_losses['bit_balance']
        losses['quantization'] = hash_reg_losses['quantization']
        losses['entropy'] = hash_reg_losses['entropy']
        losses['rate_kl'] = hash_reg_losses['rate_kl']
        losses['bit_decorrelation'] = hash_reg_losses.get('bit_decorrelation', torch.tensor(0.0, device=device))

        # æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´hashæƒé‡
        if stage_name == 'feature_focus':
            hash_weight = 0.02  # Stage1ï¼šæœ€å°hashæƒé‡ï¼Œä¸“æ³¨ç‰¹å¾é‡å»º
        elif stage_name in ['audio_weak_gan', 'progressive_gan']:
            hash_weight = 0.05  # Stage2-3ï¼šé€‚ä¸­hashæƒé‡
        else:
            hash_weight = 0.1   # Stage4ï¼šæ¢å¤æ­£å¸¸hashæƒé‡

        losses['hash_total'] = (
            hash_weight * losses['bit_balance'] +
            hash_weight * losses['quantization'] +
            hash_weight * losses['entropy'] +
            0.05 * losses['rate_kl'] +
            0.02 * losses['bit_decorrelation']
        )

    else:
        losses['bit_balance'] = torch.zeros(1, device=device)
        losses['quantization'] = torch.zeros(1, device=device)
        losses['entropy'] = torch.zeros(1, device=device)
        losses['rate_kl'] = torch.zeros(1, device=device)
        losses['bit_decorrelation'] = torch.zeros(1, device=device)
        losses['hash_total'] = torch.zeros(1, device=device)

    # 6. è‡ªåŠ¨ç¼©æ”¾å¯¹æŠ—æŸå¤±ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
    eps = 1e-6
    if current_lambda_adv > 0 and losses['adv'].item() > eps:
        adv_scale = (losses['recon'].detach() + eps) / (losses['adv'].detach() + eps)
        adv_scale = torch.clamp(adv_scale, 0.001, 0.1)
    else:
        adv_scale = torch.tensor(0.001, device=device)

    # 7. æ€»æŸå¤±ï¼ˆä½¿ç”¨æ¸è¿›å¼æƒé‡ï¼‰
    total_loss = (
        cfg.lambda_wave * losses['recon'] +
        current_lambda_adv * adv_scale * losses['adv'] +
        current_lambda_spi * losses.get('spi_total', torch.zeros(1, device=device)) +
        losses['enhanced'] +  # å¢å¼ºæŸå¤±
        losses.get('hash_total', torch.zeros(1, device=device))  # Hashæ­£åˆ™åŒ–
    )

    # æ·»åŠ å…ƒæ•°æ®
    losses.update({
        'total': total_loss,
        'adv_scale': adv_scale,
        'recon_type': recon_type,
        'stage_name': stage_name,
        'current_lambda_adv': current_lambda_adv,
        'current_lambda_spi': current_lambda_spi,
    })

    return total_loss, losses


def build_spi_model(cfg: SPITrainConfig) -> SPI_LiteSpeechJSCC:
    """æ„å»ºSPIæ¨¡å‹"""
    device = torch.device(cfg.device)

    model = SPI_LiteSpeechJSCC(
        feat_dim=20,  # åªä½¿ç”¨å£°å­¦ç‰¹å¾
        d_csi=4,
        d_z=cfg.d_z,
        d_s=cfg.d_s,
        n_bits=cfg.d_s,
        hidden=80,
        img_size=cfg.img_size,
        semantic_dim=cfg.semantic_dim,
        device=device,
    )

    # Step1ç­–ç•¥ï¼šæ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨Hash
    if not cfg.enable_hash:
        print("Hash bottleneck disabled for clean baseline testing")
        model.hash = None  # ç¦ç”¨Hashæ¨¡å—

    return model


def init_fargan_from_checkpoint(model, cfg: SPITrainConfig, device: torch.device) -> None:
    """åŠ è½½å¹¶å†»ç»“FARGANå£°ç å™¨æƒé‡ï¼ˆåªä¿®æ”¹æœ¬è„šæœ¬å†…çš„ä½¿ç”¨æ–¹å¼ï¼‰"""
    # è·å–å®é™…æ¨¡å‹ (å¤„ç†DDPåŒ…è£…)
    actual_model = model.module if hasattr(model, 'module') else model

    if not hasattr(actual_model, "vocoder"):
        print("Model has no vocoder; skip FARGAN init.")
        return

    ckpt_path = cfg.fargan_ckpt
    if not ckpt_path:
        print("No FARGAN checkpoint path provided; skip FARGAN init.")
        return

    if not os.path.isfile(ckpt_path):
        print(f"FARGAN checkpoint not found: {ckpt_path}")
        return

    print(f"Loading FARGAN weights from: {ckpt_path}")
    try:
        ckpt = torch.load(ckpt_path, map_location=device)
    except Exception as e:
        print(f"Failed to load FARGAN checkpoint: {e}")
        return

    # å°è¯•ä»å¸¸è§å­—æ®µæå–è§£ç å™¨æƒé‡
    if isinstance(ckpt, dict):
        if "decoder_state_dict" in ckpt:
            dec_sd = ckpt["decoder_state_dict"]
        elif "model_state_dict" in ckpt:
            dec_sd = ckpt["model_state_dict"]
        elif "state_dict" in ckpt:
            dec_sd = ckpt["state_dict"]
        else:
            dec_sd = ckpt
    else:
        dec_sd = ckpt

    voc_sd = actual_model.vocoder.state_dict()
    to_load = {}

    # æ˜ å°„checkpointå‚æ•°ååˆ°æ¨¡å‹å‚æ•°å
    for ckpt_name, ckpt_param in dec_sd.items():
        if not isinstance(ckpt_param, torch.Tensor):
            continue

        # å°è¯•æ·»åŠ fargan_coreå‰ç¼€è¿›è¡ŒåŒ¹é…
        model_name = f"fargan_core.{ckpt_name}"

        if model_name in voc_sd and ckpt_param.shape == voc_sd[model_name].shape:
            to_load[model_name] = ckpt_param
            print(f"  Mapping: {ckpt_name} -> {model_name} {ckpt_param.shape}")

    if not to_load:
        print("No matching FARGAN parameters found in checkpoint; vocoder left unchanged.")
    else:
        voc_sd.update(to_load)
        actual_model.vocoder.load_state_dict(voc_sd, strict=False)
        print(f"Successfully loaded {len(to_load)} FARGAN parameters into vocoder!")

    # å†»ç»“FARGANå£°ç å™¨å‚æ•°
    frozen_count = 0
    for _, param in actual_model.vocoder.named_parameters():
        if param.requires_grad:
            param.requires_grad = False
            frozen_count += 1
    print(f"FARGAN vocoder parameters frozen (count={frozen_count}).")


def unfreeze_fargan_vocoder(model, global_step: int, loss_scheduler: ProgressiveLossScheduler) -> bool:
    """æ ¹æ®æ¸è¿›å¼è°ƒåº¦å™¨è§£å†»FARGANå£°ç å™¨ (Stage 4: 5000+ steps)"""
    # è·å–å®é™…æ¨¡å‹ (å¤„ç†DDPåŒ…è£…)
    actual_model = model.module if hasattr(model, 'module') else model

    if not hasattr(actual_model, "vocoder") or loss_scheduler is None:
        return False

    # æ£€æŸ¥æ˜¯å¦è¿›å…¥Stage 4 (fargan_freeze = False)
    weights = loss_scheduler.get_weights(global_step)
    should_unfreeze = not weights['fargan_freeze']

    # ä»…åœ¨ç¬¬ä¸€æ¬¡è¿›å…¥Stage 4æ—¶è§£å†»
    if should_unfreeze and global_step == loss_scheduler.stage3_steps:
        unfrozen_count = 0
        for _, param in actual_model.vocoder.named_parameters():
            if not param.requires_grad:
                param.requires_grad = True
                unfrozen_count += 1

        if unfrozen_count > 0:
            print(f"FARGAN vocoder parameters unfrozen at step {global_step} (Stage 4) (count={unfrozen_count})")
            return True

    return False


def setup_distributed(cfg: SPITrainConfig):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    if cfg.distributed:
        dist.init_process_group(backend='nccl')
        cfg.local_rank = int(os.environ['LOCAL_RANK'])
        cfg.world_size = dist.get_world_size()
        torch.cuda.set_device(cfg.local_rank)
        if cfg.local_rank == 0:
            print(f"ğŸŒ Distributed training initialized: {cfg.world_size} GPUs")
    return cfg.local_rank == 0  # è¿”å›æ˜¯å¦ä¸ºä¸»è¿›ç¨‹


def train_spi(cfg: SPITrainConfig):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    is_main_process = setup_distributed(cfg)
    device = torch.device(f'cuda:{cfg.local_rank}' if cfg.distributed else cfg.device)

    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„åŠ è½½å™¨
    from pathlib import Path
    data_root = Path(cfg.data_root)

    # æ£€æŸ¥æ˜¯å¦æœ‰multi-expertæ•°æ®é›†
    expert_files_exist = any([
        (data_root / f"{expert}_enhanced_36.f32").exists() or
        (data_root / f"{expert}_200k_36.f32").exists()
        for expert in ["harmonic", "transient", "burst_inpaint", "low_snr"]
    ])

    if expert_files_exist:
        if is_main_process:
            print("Multi-expertæ•°æ®é›†æ£€æµ‹åˆ°ï¼Œä½¿ç”¨CombinedExpertDataset")
        dataloader, dataset = create_combined_data_loader(
            data_root=cfg.data_root,
            sequence_length=cfg.sequence_length,
            batch_size=cfg.batch_size // cfg.world_size,  # è°ƒæ•´batch_size
            max_samples=None,
            num_workers=8 // cfg.world_size,  # è°ƒæ•´workeræ•°é‡
            energy_selection=True,
        )
    else:
        if is_main_process:
            print("å•ä¸€æ•°æ®é›†æ£€æµ‹åˆ°ï¼Œä½¿ç”¨AETHERRealDataset")
        # æ£€æŸ¥åŸºç¡€æ•°æ®æ–‡ä»¶
        features_file = str(data_root / "out_features.f32")
        audio_file = str(data_root / "out_speech.pcm")

        if not (data_root / "out_features.f32").exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°ç‰¹å¾æ–‡ä»¶: {features_file}")
        if not (data_root / "out_speech.pcm").exists():
            raise FileNotFoundError(f"æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶: {audio_file}")

        dataloader, dataset = create_aether_data_loader(
            data_dir=cfg.data_root,
            sequence_length=cfg.sequence_length,
            batch_size=cfg.batch_size // cfg.world_size,  # è°ƒæ•´batch_size
            features_file=features_file,
            audio_file=audio_file,
            max_samples=None,
            num_workers=8 // cfg.world_size,  # è°ƒæ•´workeræ•°é‡
            energy_selection=True,
            feature_spec_type="fargan",  # ä½¿ç”¨FARGANå…¼å®¹çš„36ç»´ç‰¹å¾è§„èŒƒ
            distributed=cfg.distributed  # ä¼ é€’åˆ†å¸ƒå¼æ ‡å¿—
        )

    # æ„å»ºæ¨¡å‹
    model = build_spi_model(cfg)
    model.to(device)
    if is_main_process:
        model.print_model_info()

    # åŠ è½½å¹¶å†»ç»“FARGANå£°ç å™¨ï¼ˆvocoderï¼‰æƒé‡
    init_fargan_from_checkpoint(model, cfg, device)

    # åˆ†å¸ƒå¼æ¨¡å‹åŒ…è£…
    if cfg.distributed:
        model = DDP(model, device_ids=[cfg.local_rank], output_device=cfg.local_rank,
                   find_unused_parameters=True)
        if is_main_process:
            print(f"ğŸ”— Model wrapped with DistributedDataParallel")

    # ä¿¡é“æ¨¡æ‹Ÿå™¨
    channel_sim = ChannelSimulator(sample_rate=16000, frame_hz=100)

    # å¢å¼ºéŸ³é¢‘æŸå¤±ï¼ˆF0 + æ„ŸçŸ¥æŸå¤±ï¼‰
    enhanced_loss = None
    if cfg.use_enhanced_losses:
        enhanced_loss = create_enhanced_audio_loss(
            sr=16000,
            f0_weight=cfg.f0_loss_weight,
            perceptual_weight=cfg.perceptual_loss_weight,
            use_f0_loss=True,
            use_perceptual_loss=True
        ).to(device)
        print(f"Initialized enhanced losses: F0 weight={cfg.f0_loss_weight}, Perceptual weight={cfg.perceptual_loss_weight}")

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-5)

    # æ¸è¿›å¼æŸå¤±è°ƒåº¦å™¨ (Anti-Buzzæ ¸å¿ƒç»„ä»¶)
    loss_scheduler = ProgressiveLossScheduler(
        stage1_steps=1000,    # Stage 1: ä¼˜å…ˆç‰¹å¾çº§ç›‘ç£
        stage2_steps=3000,    # Stage 2: åŠ å…¥éŸ³é¢‘çº§ç›‘ç£ï¼Œå¼±GAN
        stage3_steps=5000,    # Stage 3: é€æ­¥æ¢å¤GANæƒé‡
        max_adv_weight=cfg.lambda_adv,  # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§å¯¹æŠ—æƒé‡
        max_spi_weight=cfg.lambda_spi   # ä½¿ç”¨é…ç½®ä¸­çš„æœ€å¤§SPIæƒé‡
    )

    # å¯¹æŠ—æŸå¤±å’Œåˆ¤åˆ«å™¨ (è¿›ä¸€æ­¥å¼±åŒ–)
    adv_wave_loss = create_adversarial_wave_loss(
        fft_sizes=[256],             # åªä¿ç•™ä¸€ä¸ªé¢‘ç‡å°ºåº¦
        hop_factors=4,
        base_channels=4,             # è¿›ä¸€æ­¥å‡å°‘åˆ¤åˆ«å™¨èƒ½åŠ›
        feature_match_weight=1.0,    # é™ä½ç‰¹å¾åŒ¹é…æƒé‡
        adversarial_weight=0.5,      # é™ä½å¯¹æŠ—æƒé‡
    ).to(device)

    disc_optimizer = torch.optim.AdamW(
        adv_wave_loss.get_discriminator_parameters(),
        lr=5e-5, betas=(0.5, 0.9)    # æ›´ä½çš„å­¦ä¹ ç‡
    )

    # æ¢å¤æ£€æŸ¥ç‚¹
    start_epoch = 0
    global_step = 0
    if cfg.resume_from:
        start_epoch, global_step = load_checkpoint(
            cfg.resume_from, model, optimizer, disc_optimizer, device
        )

    # è®­ç»ƒå¾ªç¯
    for epoch in range(start_epoch, cfg.num_epochs):
        model.train()

        # åˆ›å»ºå¸¦è¿›åº¦æ¡çš„dataloader
        epoch_pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{cfg.num_epochs}",
                         leave=True, dynamic_ncols=True)

        for batch in epoch_pbar:
            feats_raw = batch["x"].to(device)[..., :20]  # åªç”¨å‰20ç»´å£°å­¦ç‰¹å¾
            audio = batch["audio"].to(device)

            # ç¡®å®šè®­ç»ƒé˜¶æ®µ
            stage = determine_training_stage(global_step, cfg)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è§£å†»FARGAN (æ ¹æ®æ¸è¿›å¼è°ƒåº¦å™¨)
            if unfreeze_fargan_vocoder(model, global_step, loss_scheduler):
                # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ä»¥åŒ…å«æ–°è§£å†»çš„å‚æ•°
                optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-5)
                tqdm.write(f"Step {global_step}: FARGAN vocoder unfrozen, optimizer recreated")

            # æ£€æŸ¥é˜¶æ®µåˆ‡æ¢
            current_stage_name = loss_scheduler.get_weights(global_step)['stage_name']
            prev_stage_name = loss_scheduler.get_weights(global_step - 1)['stage_name'] if global_step > 0 else ''
            if current_stage_name != prev_stage_name:
                tqdm.write(f"Step {global_step}: Training stage switched to '{current_stage_name}'")

            # === SPIå‰å‘ä¼ æ’­ ===
            spi_output = forward_spi_progressive(
                model=model,
                feats=feats_raw,
                audio=audio,
                channel_sim=channel_sim,
                stage=stage,
                cfg=cfg,
                device=device,
            )

            # æ·»åŠ åŸå§‹ç‰¹å¾åˆ°è¾“å‡º (ç”¨äºæŸå¤±è®¡ç®—)
            spi_output['feats'] = feats_raw

            audio_hat = spi_output['audio_hat']
            audio_real = spi_output['audio_real']

            # === åˆ¤åˆ«å™¨æ­¥éª¤ ===
            if adv_wave_loss is not None and global_step % 3 == 0:  # æ¯3æ­¥è®­ç»ƒ1æ¬¡åˆ¤åˆ«å™¨
                disc_optimizer.zero_grad()
                try:
                    disc_out = adv_wave_loss.discriminator_step(audio_real, audio_hat.detach())
                    loss_d = disc_out["discriminator_loss"]
                    loss_d.backward()
                    disc_optimizer.step()
                except:
                    loss_d = torch.zeros(1, device=device)
            else:
                loss_d = torch.zeros(1, device=device)

            # === ç”Ÿæˆå™¨æ­¥éª¤ ===
            optimizer.zero_grad()

            total_loss, loss_dict = compute_spi_training_loss(
                spi_output, cfg, adv_wave_loss, enhanced_loss, device, model,
                global_step=global_step, loss_scheduler=loss_scheduler
            )

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)
            optimizer.step()

            # === æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯ ===
            # è·å–å½“å‰è°ƒåº¦å™¨ä¿¡æ¯
            scheduler_weights = loss_scheduler.get_weights(global_step)

            # æ„å»ºè¿›åº¦æ¡postfixä¿¡æ¯
            postfix_dict = {
                'step': global_step,
                'stage': scheduler_weights['stage_name'][:8],  # ç¼©çŸ­æ˜¾ç¤º
                'total': f"{total_loss.item():.3f}",
                'recon': f"{loss_dict['recon'].item():.3f}",
                'adv': f"{loss_dict['adv'].item():.3f}",
                'spi': f"{loss_dict.get('spi_total', torch.zeros(1)).item():.3f}",
                'fargan': 'unfreeze' if not scheduler_weights['fargan_freeze'] else 'frozen'
            }

            # æ·»åŠ Anti-Buzzç‰¹å®šæŸå¤±ä¿¡æ¯
            if 'feat_recon' in loss_dict:
                postfix_dict['feat'] = f"{loss_dict['feat_recon'].item():.3f}"
            if 'enhanced' in loss_dict and loss_dict['enhanced'].item() > 0:
                postfix_dict['f0'] = f"{loss_dict.get('enhanced_f0', torch.zeros(1)).item():.3f}"

            epoch_pbar.set_postfix(postfix_dict)

            # === ä¿å­˜æ£€æŸ¥ç‚¹ ===
            if global_step % cfg.save_every_steps == 0 and global_step > 0:
                save_checkpoint(
                    model=model,
                    optimizer=optimizer,
                    disc_optimizer=disc_optimizer,
                    cfg=cfg,
                    epoch=epoch,
                    global_step=global_step,
                    output_dir=cfg.output_dir,
                )
                tqdm.write(f"Step {global_step}: Checkpoint saved to {cfg.output_dir}")

            # === ä¿å­˜éŸ³é¢‘æ ·æœ¬ ===
            if cfg.save_audio and global_step % cfg.save_audio_every_steps == 0:
                save_audio_samples(
                    audio_real=audio_real,
                    audio_gen=audio_hat,
                    cfg=cfg,
                    epoch=epoch,
                    global_step=global_step,
                )
                tqdm.write(f"Step {global_step}: Audio samples saved")

            # === ä¿å­˜å¯è§†åŒ–å¯¹æ¯”å›¾ ===
            if cfg.save_visualization and global_step % cfg.save_visualization_every_steps == 0:
                try:
                    # åˆ›å»ºF0å’ŒMelè°±å›¾å¯¹æ¯”
                    create_batch_comparison_plots(
                        audio_real_batch=audio_real,
                        audio_gen_batch=audio_hat,
                        save_dir=cfg.visualization_save_dir,
                        step=global_step,
                        max_samples=cfg.max_visualization_samples,
                        sr=16000
                    )

                    # åŒæ—¶ä¿å­˜éŸ³é¢‘æ–‡ä»¶ç”¨äºæ’­æ”¾éªŒè¯
                    save_comparison_audio_samples(
                        audio_real_batch=audio_real,
                        audio_gen_batch=audio_hat,
                        save_dir=cfg.visualization_save_dir,
                        step=global_step,
                        max_samples=cfg.max_visualization_samples,
                        sr=16000
                    )

                    tqdm.write(f"Step {global_step}: Visualization saved")

                except Exception as e:
                    tqdm.write(f"Step {global_step}: Failed to save visualization: {e}")

            global_step += 1

        # Epochå®Œæˆæ¶ˆæ¯
        tqdm.write(f"Epoch {epoch+1} completed!")

    tqdm.write("SPI Anti-Buzz training completed!")


def parse_args() -> SPITrainConfig:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="SPI-JSCC training")

    parser.add_argument("--data_root", type=str, default="/home/bluestar/FARGAN/opus/dnn/torch/Aether-lite/data_expert_augmented_small200k")
    parser.add_argument("--stage", type=int, choices=[2, 3, 4], default=2)
    parser.add_argument("--enable_hash", action="store_true", default=False, help="Enable Hash bottleneck (Step1: start with False)")
    parser.add_argument(
        "--fargan_ckpt",
        type=str,
        default="/home/bluestar/FARGAN/opus/dnn/torch/Aether-lite/fargan_pt/fargan_sq1Ab_adv_50.pth",
        help="FARGAN é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆå°†è¢«åŠ è½½åˆ°vocoderå¹¶å†»ç»“ï¼‰",
    )
    parser.add_argument("--fargan_unfreeze_steps", type=int, default=15000, help="FARGANè§£å†»æ­¥æ•°")
    parser.add_argument("--batch_size", type=int, default=8)  # ç¨å¾®å‡å°æ‰¹æ¬¡
    parser.add_argument("--sequence_length", type=int, default=200)
    parser.add_argument("--num_epochs", type=int, default=40)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")

    # ä¿å­˜ç›¸å…³
    parser.add_argument("--output_dir", type=str, default="./checkpoints_spi_jscc")
    parser.add_argument("--save_every_steps", type=int, default=500)
    parser.add_argument("--resume_from", type=str, default=None)

    # éŸ³é¢‘éªŒè¯
    parser.add_argument("--save_audio", action="store_true", default=True)
    parser.add_argument("--audio_save_dir", type=str, default="./audio_samples_spi")
    parser.add_argument("--save_audio_every_steps", type=int, default=100)
    parser.add_argument("--max_audio_samples", type=int, default=4)

    # å¯è§†åŒ–
    parser.add_argument("--save_visualization", action="store_true", default=True)
    parser.add_argument("--visualization_save_dir", type=str, default="./visualizations_spi")
    parser.add_argument("--save_visualization_every_steps", type=int, default=100)
    parser.add_argument("--max_visualization_samples", type=int, default=3)

    # SPIå‚æ•°
    parser.add_argument("--img_size", type=int, default=64)
    parser.add_argument("--semantic_dim", type=int, default=16)
    parser.add_argument("--d_z", type=int, default=32)
    parser.add_argument("--d_s", type=int, default=24)

    # æ¸è¿›å¼è®­ç»ƒ
    parser.add_argument("--use_progressive", action="store_true", default=True)
    parser.add_argument("--stage1_steps", type=int, default=200)
    parser.add_argument("--stage2_steps", type=int, default=400)
    parser.add_argument("--stage3_steps", type=int, default=600)

    # æŸå¤±æƒé‡ - å¹²å‡€baselineé»˜è®¤é…ç½®
    parser.add_argument("--lambda_wave", type=float, default=1.0)
    parser.add_argument("--lambda_adv", type=float, default=0.0)
    parser.add_argument("--lambda_spi", type=float, default=0.5)
    parser.add_argument("--use_enhanced_losses", action="store_true", default=False, help="Use enhanced F0 and perceptual losses")
    parser.add_argument("--f0_loss_weight", type=float, default=1.0, help="Weight for F0 consistency loss")
    parser.add_argument("--perceptual_loss_weight", type=float, default=0.1, help="Weight for perceptual losses")

    # å…¶ä»–
    parser.add_argument("--use_stft_loss", action="store_true", default=True)
    parser.add_argument("--snr_min_db", type=float, default=-5.0)
    parser.add_argument("--snr_max_db", type=float, default=5.0)

    # å¤šGPUæ”¯æŒ
    parser.add_argument("--distributed", action="store_true", help="Enable distributed training")
    parser.add_argument("--local_rank", type=int, default=0, help="Local rank for distributed training")
    parser.add_argument("--world_size", type=int, default=1, help="World size for distributed training")

    args = parser.parse_args()

    return SPITrainConfig(
        data_root=args.data_root,
        stage=args.stage,
        fargan_ckpt=args.fargan_ckpt,
        fargan_unfreeze_steps=args.fargan_unfreeze_steps,
        batch_size=args.batch_size,
        sequence_length=args.sequence_length,
        num_epochs=args.num_epochs,
        lr=args.lr,
        device=args.device,
        output_dir=args.output_dir,
        save_every_steps=args.save_every_steps,
        resume_from=args.resume_from,
        save_audio=args.save_audio,
        audio_save_dir=args.audio_save_dir,
        save_audio_every_steps=args.save_audio_every_steps,
        max_audio_samples=args.max_audio_samples,
        save_visualization=args.save_visualization,
        visualization_save_dir=args.visualization_save_dir,
        save_visualization_every_steps=args.save_visualization_every_steps,
        max_visualization_samples=args.max_visualization_samples,
        img_size=args.img_size,
        semantic_dim=args.semantic_dim,
        d_z=args.d_z,
        d_s=args.d_s,
        use_progressive=args.use_progressive,
        stage1_steps=args.stage1_steps,
        stage2_steps=args.stage2_steps,
        stage3_steps=args.stage3_steps,
        lambda_wave=args.lambda_wave,
        lambda_adv=args.lambda_adv,
        lambda_spi=args.lambda_spi,
        use_enhanced_losses=args.use_enhanced_losses,
        f0_loss_weight=args.f0_loss_weight,
        perceptual_loss_weight=args.perceptual_loss_weight,
        use_stft_loss=args.use_stft_loss,
        snr_min_db=args.snr_min_db,
        snr_max_db=args.snr_max_db,
        distributed=args.distributed,
        local_rank=args.local_rank,
        world_size=args.world_size,
    )


if __name__ == "__main__":
    cfg = parse_args()
    train_spi(cfg)
