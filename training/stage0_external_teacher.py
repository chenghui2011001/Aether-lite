#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage 0: å¤–éƒ¨è¶…å¤§è€å¸ˆç‰¹å¾æå–

åŸºäºç”¨æˆ·æŠ€æœ¯æ–¹æ¡ˆçš„ä¸‰å±‚Teacher-Studentæ¶æ„ï¼š
- å±‚0ï¼šå¤–éƒ¨è¶…å¤§è€å¸ˆï¼ˆStableCodecç­‰ï¼‰
- ç”¨é€”ï¼šç¦»çº¿æå–é«˜è´¨é‡latentå’Œæ³¢å½¢ï¼Œä¸ºåç»­è®­ç»ƒæä¾›è’¸é¦ç›®æ ‡

å…³é”®åŠŸèƒ½ï¼š
1. ä½¿ç”¨StableCodecæå–é«˜ç»´latentï¼ˆ128-256ç»´ï¼‰
2. ç”Ÿæˆé«˜è´¨é‡é‡å»ºæ³¢å½¢wav_teacher
3. å­˜å‚¨è¯­ä¹‰ç‰¹å¾ç”¨äºStage3ç›‘ç£
4. ä¸ºAether-Baseæä¾›è’¸é¦ç›®æ ‡
"""

from __future__ import annotations
import argparse
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
import numpy as np

import torch
import torch.nn as nn
import tqdm
import soundfile as sf

# ä½¿ç”¨æœ¬åœ°ç§»æ¤çš„ç»„ä»¶
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.stablecodec_teacher import StableCodecTeacher
from models.semantic_extractor import create_semantic_extractor
from utils.real_data_loader import create_aether_data_loader


class ExternalTeacherProcessor:
    """å¤–éƒ¨è¶…å¤§è€å¸ˆå¤„ç†å™¨"""

    def __init__(self,
                 stablecodec_model: str = "stabilityai/stable-audio-open-1.0",
                 device: str | None = None):

        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = torch.device(device)

        # åˆå§‹åŒ–StableCodec teacher
        # StableCodecTeacher ä½¿ç”¨ pretrained_model å…³é”®å­—æ¥æŒ‡å®šé¢„è®­ç»ƒæƒé‡æ ‡è¯†
        self.stablecodec = StableCodecTeacher(
            pretrained_model=stablecodec_model,
            device=self.device
        )

        # åˆå§‹åŒ–è¯­ä¹‰æå–å™¨ï¼ˆä½¿ç”¨StableCodecä½œä¸ºSSLæ¨¡å‹ï¼‰
        self.semantic_extractor = create_semantic_extractor(
            model_name="stablecodec",  # ä½¿ç”¨StableCodec
            proj_dim=16,
            device=self.device
        )

        print(f"âœ… External Teacher initialized:")
        print(f"   StableCodec: {stablecodec_model}")
        print(f"   Semantic Extractor: StableCodec-based")
        print(f"   Device: {self.device}")

    @torch.no_grad()
    def process_audio_batch(self, audio_batch: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        å¤„ç†ä¸€ä¸ªbatchçš„éŸ³é¢‘ï¼Œæå–å¤–éƒ¨è€å¸ˆç‰¹å¾

        Args:
            audio_batch: [B, L] éŸ³é¢‘æ³¢å½¢

        Returns:
            å­—å…¸åŒ…å«ï¼š
            - latent_teacher: [B, T_latent, D_latent] é«˜ç»´latent
            - wav_teacher: [B, L] é‡å»ºéŸ³é¢‘
            - semantic_teacher: [B, T_frames, 16] è¯­ä¹‰ç‰¹å¾
        """
        B, L = audio_batch.shape
        audio_batch = audio_batch.to(self.device)

        # 1. StableCodecç¼–ç -è§£ç 
        stablecodec_output = self.stablecodec.encode_decode(audio_batch)
        latent_teacher = stablecodec_output['latent']  # [B, T_latent, D_latent]
        wav_teacher = stablecodec_output['reconstructed']  # [B, L]

        # 2. è¯­ä¹‰ç‰¹å¾æå–ï¼ˆä½¿ç”¨åŸå§‹éŸ³é¢‘ï¼‰
        # ä¼°ç®—å¸§æ•°ï¼š16kHzéŸ³é¢‘ï¼Œ100Hzå¸§ç‡
        target_frames = int(L * 100 / 16000)
        semantic_teacher = self.semantic_extractor(audio_batch, target_frames=target_frames)

        return {
            'latent_teacher': latent_teacher.cpu(),
            'wav_teacher': wav_teacher.cpu(),
            'semantic_teacher': semantic_teacher.cpu()
        }

    def process_dataset(self,
                       data_loader,
                       output_dir: str,
                       max_batches: Optional[int] = None):
        """
        å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œç”Ÿæˆå¤–éƒ¨è€å¸ˆç‰¹å¾

        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            output_dir: è¾“å‡ºç›®å½•
            max_batches: æœ€å¤§å¤„ç†batchæ•°ï¼ˆNoneä¸ºå…¨éƒ¨ï¼‰
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            'total_batches': 0,
            'total_samples': 0,
            'latent_dim': None,
            'semantic_dim': 16,
            'sample_rate': 16000
        }

        print(f"ğŸš€ å¼€å§‹å¤„ç†æ•°æ®é›†ï¼Œè¾“å‡ºåˆ°: {output_dir}")

        for batch_idx, batch in enumerate(tqdm.tqdm(data_loader, desc="Processing batches")):
            if max_batches is not None and batch_idx >= max_batches:
                break

            # æå–éŸ³é¢‘ï¼ˆå‡è®¾batchåŒ…å«audioï¼‰
            if isinstance(batch, dict):
                audio = batch['audio']
            else:
                audio = batch[0]  # å‡è®¾ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯éŸ³é¢‘

            # å¤„ç†batch
            teacher_features = self.process_audio_batch(audio)

            # æ›´æ–°ç»Ÿè®¡
            stats['total_batches'] += 1
            stats['total_samples'] += audio.size(0)
            if stats['latent_dim'] is None:
                stats['latent_dim'] = teacher_features['latent_teacher'].size(-1)

            # ä¿å­˜ç‰¹å¾
            batch_output_file = output_dir / f"teacher_features_batch_{batch_idx:06d}.npz"

            # è½¬æ¢ä¸ºnumpyä¿å­˜
            features_np = {}
            for key, tensor in teacher_features.items():
                features_np[key] = tensor.numpy()

            np.savez_compressed(batch_output_file, **features_np)

            # å®šæœŸæ‰“å°è¿›åº¦
            if batch_idx % 100 == 0:
                print(f"   Processed {batch_idx} batches, {stats['total_samples']} samples")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = output_dir / "stats.json"
        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)

        print(f"âœ… å¤„ç†å®Œæˆ:")
        print(f"   æ€»batchæ•°: {stats['total_batches']}")
        print(f"   æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"   Latentç»´åº¦: {stats['latent_dim']}")
        print(f"   è¯­ä¹‰ç»´åº¦: {stats['semantic_dim']}")
        print(f"   ç‰¹å¾æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="Stage 0: External Teacher Feature Extraction")
    parser.add_argument("--features", type=str, required=True, help="Input features file (.f32)")
    parser.add_argument("--pcm", type=str, required=True, help="Input PCM audio file")
    parser.add_argument("--output_dir", type=str, required=True, help="Output directory for teacher features")
    parser.add_argument("--batch_size", type=int, default=8, help="Batch size")
    parser.add_argument("--max_batches", type=int, default=None, help="Max batches to process (for testing)")
    parser.add_argument("--stablecodec_model", type=str,
                       default="stabilityai/stable-audio-open-1.0",
                       help="StableCodec model name")
    parser.add_argument("--device", type=str, default=None, help="Device (cuda/cpu)")

    args = parser.parse_args()

    print("=" * 60)
    print("ğŸ¯ Stage 0: External Teacher Feature Extraction")
    print("=" * 60)
    print(f"è¾“å…¥ç‰¹å¾: {args.features}")
    print(f"è¾“å…¥éŸ³é¢‘: {args.pcm}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"Batchå¤§å°: {args.batch_size}")
    print(f"StableCodecæ¨¡å‹: {args.stablecodec_model}")

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\nğŸ“‚ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    data_loader = create_aether_data_loader(
        features_file=args.features,
        pcm_file=args.pcm,
        batch_size=args.batch_size,
        shuffle=False  # Stage0ä¸éœ€è¦shuffleï¼Œä¿æŒé¡ºåº
    )

    print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œæ€»batchæ•°: {len(data_loader)}")

    # åˆ›å»ºå¤–éƒ¨è€å¸ˆå¤„ç†å™¨
    print("\nğŸ§  åˆå§‹åŒ–å¤–éƒ¨è€å¸ˆ...")
    processor = ExternalTeacherProcessor(
        stablecodec_model=args.stablecodec_model,
        device=args.device
    )

    # å¤„ç†æ•°æ®é›†
    print("\nğŸ”„ å¼€å§‹ç‰¹å¾æå–...")
    processor.process_dataset(
        data_loader=data_loader,
        output_dir=args.output_dir,
        max_batches=args.max_batches
    )

    print("\n" + "=" * 60)
    print("Stage 0 completed successfully")
    print("=" * 60)


if __name__ == "__main__":
    main()
